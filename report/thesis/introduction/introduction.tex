\graphicspath{{introduction/}}

\chapter{Introduction}
\label{cha:introduction}

Finding correspondences between recent and formerly visited camera frames is a critical component in many computer vision and robotics systems. For the problem of simultaneous localization and mapping (SLAM), camera relocalization establishes loop closures, which are necessary to correct drift accumulated during the estimate. For applications which do not perform mapping, such as visual odometry (tracking camera movement from video), being able to relocalize is critical to recovering from tracking loss and returning to a known reference frame. This is also relevant to applications on low-power devices, such as mobile robotics or virtual and augmented reality on smartphones, in which it may be desirable to compute a high-quality map ahead of time and track against it later. In some use-cases, such as image retrieval, identifying the pose (i.e.\ the position and orientation) of the viewer in a known map is the goal itself.

In this thesis, we address the problem of relocalization using a recently developed measurement representation called ``semi-dense'' depth maps. In contrast to sparse depth maps, which only store a few nonzero values, and fully dense depth maps, which have a distance stored in every pixel, semi-dense depth maps record the depth of a \textit{significant fraction} of pixels. Recent developments in direct visual odometry \cite{engel2013semi} \cite{engel2017direct} \cite{Forster2014ICRA} have been able to achieve comparable results to depth-based methods, despite only using an RGB camera, and a recurring theme in these methods is the estimation of a semi-dense depth map in each frame. In these works, the maps contain a depth estimate at every pixel with a significant color gradient. This is in contrast to feature-based techniques \cite{klein2007parallel} \cite{murTRO2015}, which only track points located at peaks in the feature response space, and produce a comparatively sparse point cloud. In this thesis, we show how state-of-the-art RGB-D relocalization techniques can be re-applied RGB images paired with semi-dense depth maps.

Monocular (single camera) visual odometry itself comes with a variety of challenges. For example, depth estimation is indeterminate in the case of zero (or very little) camera translation. This can happen if a user rotates a camera around the camera center, perhaps to give a panoramic view of the scene. When camera translation resumes, if no original points are still visible within the viewing frustum, tracking is impossible. Additionally, in lieu of feature matching, direct techniques must make assumptions about camera motion to track pixels from frame to frame. They generally assume small camera movement, and only attempt to track points within a small radius, or (using a motion model) they might search on an epipolar line for the best point correspondence. These assumptions are violated in the case of fast or shaky camera movement, which is common in the real world and further motivates our problem.

Furthermore, these common sources of tracking failure are exacerbated by the scale ambiguity of video from a single camera. In the absence of inertial measurements or calibrated depth cameras, it is impossible to know the true size of observed objects. For example, in a typical structure-from-motion pipeline, the displacement between the initially chosen frames defines the scale. Because of this, multiple independent reconstructions of the same scene may be at arbitrarily different scales. In an on-line system, if tracking fails, we must recover both the original reference frame and the original scale of the scene.

In Chapter 2, we give an overview of related work. Chapter 3 gives a detailed description of the relocalization pipeline. This includes a summary of the visual odometry operation and depth map extraction techniques, as well as an overview of a typical image-based relocalization process. We further expound on techniques for descriptor learning and full \mbox{6-degree-of-freedom} camera localization. Chapter 4 presents concrete results on relocalization benchmarks, and we compare our work to prior visual and depth-based relocalization methods. We conclude in Chapter 5 with a discussion of future work, applications, limitations, and possible extensions to the method.

\cleardoublepage
