

\chapter{Related Work}



This thesis covers a small portion of the much broader field of camera relocalization. This field addresses a conceptually simple problem: if we have viewed a scene with a camera, and then we return to the same scene later, we would like to figure out where we are. Naturally though, depending on the size and type of the scene, the available sensor hardware, and constraints on performance, accuracy, and runtime, this may require a variety of solutions.

At the highest level, camera relocalization can simply be cast as an image retrieval problem. If we have a database of images labeled with poses, i.e.\ position and rotation, it may be sufficient to simply find a similar image and return that camera's pose. But how do we rate similarity? In their landmark work, Sivic and Zisserman \cite{sivic2003video} answered this by recasting the problem as one of text retrieval, in a so-called ``Video Google'' system. In their work, each image is represented as a set of ``visual words''---which is to say, meaningful predetermined vectors that describe the image. These descriptors are analogous to words in a text document, and as a result, one can compare image similarity using the \textit{term frequency-inverse document frequency} metric commonly used in text search, and the images can be indexed using standard text retrieval techniques. In Sivic and Zisserman's work, they use SIFT descriptors \cite{lowe1999object}, which are desirable due to their invariance to a variety of common camera transforms.

This quantization of images and descriptors into a ``bag of visual words'' has been revisited in many subsequent works. More recently, Philbin \text{et al}.\ \cite{philbin2007object} demonstrate methods to increase the visual vocabulary size to improve the discriminative power of visual words, and they also take into account the original geometric configuration of features with a spatial re-ranking step. Orthogonally, J{\'e}gou \textit{et al}.\ \cite{jegou2008hamming} introduce the use of Hamming embeddings for refining the accuracy of visual words. This is further expounded upon by Arandjelovi\'c and Zisserman \cite{Arandjelovic14a}, and Sattler \textit{et al}.\ \cite{Sattler16CVPR}, who additionally re-rank the top $N$ retrieved results to normalize for common spatial configurations of features.

In addition, J{\'e}gou \textit{et al}.\ \cite{jegou2010aggregating} augment the visual words histogram. They calculate a vector describing the average offset from a visual word center to the actual descriptors present in an image, which they denote the Vector of Locally Aggregated Descriptors (VLAD). Later work \cite{arandjelovic2016netvlad} incorporates this aspect into a deep descriptor learning framework.

In effect, these quantization and index lookup techniques compute a hash of image features that preserves similarity in the descriptor space. There are other ways to compute this as well, for example locality sensitive hashing \cite{datar2004locality}; however, the visual word index approach tends to be dominant due to its small memory footprint compared to other techniques.

Of course, the pose of a database camera is only a rough approximation of the query camera's true pose. If positions of 3D points in the original scene are known, as may be case in a typical structure-from-motion pipeline, we can significantly improve the result by estimating the transform from the retrieved camera to the query camera. This pose registration process is frequently performed in two steps. In the first step, one finds point correspondences between descriptors in both images; in general, this is done by matching descriptor pairs that are nearest neighbors of each other, and may be refined using a ratio test as in \cite{lowe1999object}. This is followed by a model fitting step, in which one finds a relative pose that aligns as many point correspondences as possible. Because many selected points may be outliers, and will not be aligned by the truth model, this is generally performed using a RANSAC loop \cite{fischler1981random}, in which the minimum number of correspondences needed to estimate the pose is repeatedly sampled at random, and the estimated model with the largest consensus set is kept.

This process---image retrieval, point matching, and robust estimation---is sometimes bypassed in favor of direct matching techniques. In this case, all 3D points in the scene are considered as possible matches. This has the potential for increased localization accuracy, since it considers more viable point correspondences. The drawback to this strategy is that it requires a larger memory footprint, since one must perform a search for matching descriptors rather than matching visual vocabulary histograms (which could be easily indexed and stored on disk). In several works \cite{li2010location} \cite{sattler2011fast} \cite{sattler2012image}, proponents explore a prioritized searching scheme to garner an improvement in accuracy while retaining a fast runtime. In this case, the omitted image retrieval step could be seen as a proxy to detect 3D points that ought to be co-visible from a single viewpoint.

Furthermore, these works investigate large-scale urban datasets, which are extremely challenging due to the huge variety of visual imagery. In smaller scenes, such as those investigated by \cite{schmidt2017self}, it may be viable to perform direct matching without any sort of prioritized search.

A variety of newer work eschews the traditional image retrieval process and seeks to provide faster and more task-specific retrieval solutions. Particularly due to the commoditization of depth-sensing cameras (like the Kinect), many works perform relocalization on RGB-D images. Glocker \textit{et al}.\ \cite{glocker2013real} \cite{glocker2015real} relocalize by applying randomized ferns (randomized forests of decision tree stumps) to images to generated encodings, and returning nearest neighbor database images. Shotton \textit{et al}.\ \cite{shotton2013scene} perform regression to directly predict the 3D world coordinate of pixels in a single image using randomized forests, and then perform RANSAC pose estimation on the result. Valentin \textit{et al}.\ \cite{valentin2015exploiting} further improve this by predicting the uncertainty of the random forest regression to refine the final pose estimate. In Schmidt \textit{et al}.\ \cite{schmidt2017self}, the authors learn a dense feature descriptor to use for point correspondence. They leverage prior reconstructions of the scene to automate the task of finding training samples.

Some literature seeks to find descriptors that embody the geometric properties of the scene. These have the advantage of being invariant to changes in lighting conditions and camera viewing angles. For example, Rusu \textit{et al}.\ \cite{rusu2009fast} present a hand-crafted geometric descriptor based on the local normals and curvatures of points in a 3D pointcloud. More recently, Zeng \textit{et al}.\ \cite{zeng20163dmatch} present 3DMatch, which learns a geometric descriptor using a setup similar to \cite{schmidt2017self}.

Still other works seek to use machine learning to replace the pose estimation step. This would be desirable, since RANSAC can be slow in the case of large outlier ratios or high model complexity, and requires some manual tuning. Kendall \textit{et al}.\ \cite{kendall2015posenet} directly regress on the 6-DoF camera pose using a deep neural net, which takes only milliseconds to evaluate. They expand on this idea in later publications \cite{kendall2016modelling} \cite{kendall2017geometric}. In \cite{brachmann2014learning} and \cite{brachmann2016dsac}, the authors learn an alternative point correspondence sampling scheme and hypothesis scoring scheme to replace the methods used in vanilla RANSAC.

In this thesis, we specifically explore the topic of relocalization using semi-dense depth data. In contrast to almost all the previously mentioned work, we assume that a depth camera is unavailable, and that rough, partially empty depth maps have been generated through the use of a purely visual tracking pipeline. Such a method was pioneered by Engel \textit{et al}.\ \cite{engel2013semi}, who proposed it to do visual odometry. In their work, they alternate between depth map propagation and dense tracking using prior depth estimates. Their key insight is that, given an initial depth map and the relative pose to the next frame, they can perform an epipolar line search along each depth ray originating from the first frame to find a likely depth value in the second frame. If we assume that both the inverse depth value from the first frame and the inverse depth value from the second frame are Gaussian-distributed, we can combine the noisy estimates in the manner of an extended Kalman filter.

Engel \textit{et al}.'s work is particularly seminal because it produced high-quality visual odometry by directly minimizing the photometric error between frames, while running as fast as depth-based and feature-based approaches. In a continuation of this work, Engel \textit{et al}.\ published LSD-SLAM \cite{engel2014lsd}, which expresses the frame-to-frame transformations using pose graph in order to perform global bundle adjustment and facilitate large-scale loop closures. Monocular camera odometry drifts in scale over time; therefore, this publication was also notable for the use of similarity constraints (rather than constraints on rigid motion) between poses in the pose graph. To perform large-scale loop closure the authors use OpenFABMAP \cite{glover2012openfabmap}, which is functionally similar to the traditional image retrieval pipelines mentioned earlier. More recently, Engel \textit{et al}.\ released DSO \cite{engel2017direct}, which additionally estimates affine changes in lighting conditions. Another work, SVO \cite{forster2014svo} \cite{forster2017svo} addresses a similar solution.

\cleardoublepage
